{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from zeugma.embeddings import EmbeddingTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class Tokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.pt = PorterStemmer()\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.tk = RegexpTokenizer(r'\\b[a-zA-Z]{3,}\\b')\n",
    "        self.stpwrd = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in self.tk.tokenize(doc) if not t in self.stpwrd]\n",
    "\n",
    "my_tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PorterStemmer>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tokenizer.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "token = RegexpTokenizer(r'\\b[a-zA-Z]{3,}\\b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "        'This is the first document.',\n",
    "        'This is the second second document document document document document.',\n",
    "        'And the third ones and one.',\n",
    "        'Is this the first document documents document?',\n",
    "        'hey I HATE YOU www.google.com 12343 document document',\n",
    "        'Just kidding my name is kim i love loves you document'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "glove = EmbeddingTransformer('glove')\n",
    "x_train = glove.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.67724018e-02,  1.15701340e-01,  2.37286672e-01,\n",
       "        -3.52290004e-01, -2.82916665e-01, -1.72041342e-01,\n",
       "         1.58946669e+00, -6.59300014e-02, -1.06396675e-01,\n",
       "        -2.67053336e-01,  2.40593329e-01,  8.97736728e-01,\n",
       "        -5.74183321e+00,  3.09773356e-01,  3.12203318e-01,\n",
       "        -8.59766603e-02,  4.40383345e-01,  1.22636460e-01,\n",
       "        -3.92583340e-01,  1.69833396e-02, -5.15653312e-01,\n",
       "         3.35700005e-01, -5.34686632e-02, -2.85056680e-01,\n",
       "        -3.17292988e-01],\n",
       "       [ 3.82314146e-01,  2.78517991e-01, -1.36913747e-01,\n",
       "        -5.64557612e-02,  3.08194757e-01, -6.36742949e-01,\n",
       "         7.78987408e-01, -6.47161245e-01,  5.30776501e-01,\n",
       "        -4.45980012e-01,  4.01381254e-01,  7.83620059e-01,\n",
       "        -3.71313715e+00,  2.90478349e-01,  3.94148767e-01,\n",
       "         3.68597686e-01,  4.89348769e-01,  5.68204880e-01,\n",
       "        -3.12554955e-01, -1.97042495e-01, -5.02723694e-01,\n",
       "        -5.24401248e-01, -9.56712663e-02, -2.90639997e-01,\n",
       "        -7.31678665e-01],\n",
       "       [ 2.90977955e-02,  2.48932794e-01,  3.76585394e-01,\n",
       "        -1.56360976e-02, -1.56629995e-01, -1.88265994e-01,\n",
       "         1.15426958e+00, -8.40061963e-01,  1.32568002e-01,\n",
       "        -2.13157848e-01,  2.54880004e-02,  3.57863009e-01,\n",
       "        -4.20535994e+00,  3.94138396e-01,  9.41622443e-03,\n",
       "        -1.53222010e-01,  3.92798007e-01, -1.97742134e-01,\n",
       "         4.56427991e-01, -5.36774397e-02, -5.04199982e-01,\n",
       "         4.08199966e-01,  1.31339997e-01, -2.76478440e-01,\n",
       "        -6.89303994e-01],\n",
       "       [ 3.50520551e-01,  2.84360796e-01,  5.71250096e-02,\n",
       "        -9.73828062e-02,  1.71129793e-01, -7.40101993e-01,\n",
       "         1.12102997e+00, -6.41744018e-01,  2.99416006e-01,\n",
       "        -3.45956028e-01,  2.76149988e-01,  6.09227955e-01,\n",
       "        -4.29487991e+00,  2.22587198e-01,  3.25313985e-01,\n",
       "         5.79819903e-02,  5.46844006e-01,  1.39155880e-01,\n",
       "        -2.72042006e-01, -2.04353809e-01, -5.13606369e-01,\n",
       "        -4.49302018e-01, -4.16519027e-03, -4.58864391e-01,\n",
       "        -6.23898029e-01],\n",
       "       [ 4.62904334e-01,  4.27980036e-01, -1.86196670e-01,\n",
       "         9.29373279e-02,  3.34333330e-01, -8.73036623e-01,\n",
       "         5.75333357e-01, -3.31533343e-01,  4.81666684e-01,\n",
       "        -3.13339978e-01,  2.01116681e-01,  4.00213331e-01,\n",
       "        -2.61083341e+00, -3.34344000e-01,  3.17015678e-01,\n",
       "         3.92999977e-01,  5.59273303e-01,  5.53839982e-01,\n",
       "        -4.45760012e-01, -4.97666979e-03,  2.71133389e-02,\n",
       "        -8.92395318e-01, -4.44876671e-01,  4.43266630e-02,\n",
       "        -6.71383381e-01],\n",
       "       [-2.56272018e-01,  1.74272567e-01,  2.69421697e-01,\n",
       "         7.64866024e-02, -3.55277002e-01, -4.91369188e-01,\n",
       "         1.23547101e+00,  1.61091909e-01, -5.78952610e-01,\n",
       "         7.55594820e-02, -3.63070011e-01,  4.67958599e-01,\n",
       "        -4.60393047e+00, -4.79328632e-01, -3.06865990e-01,\n",
       "         1.01827398e-01,  4.34532732e-01, -2.67768919e-01,\n",
       "        -3.86001974e-01,  7.98619986e-02,  3.17132056e-01,\n",
       "         2.53614724e-01, -3.50000679e-01,  2.62726486e-01,\n",
       "        -4.97325897e-01]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['com',\n",
       " 'document',\n",
       " 'first',\n",
       " 'google',\n",
       " 'hate',\n",
       " 'hey',\n",
       " 'kidding',\n",
       " 'kim',\n",
       " 'love',\n",
       " 'name',\n",
       " 'one',\n",
       " 'second',\n",
       " 'third',\n",
       " 'www']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(\n",
    "#                            analyzer='word',\n",
    "                           \n",
    "                      lowercase = True,\n",
    "#                       stop_words=stop_words,\n",
    "                      token_pattern=None,\n",
    "                      tokenizer=my_tokenizer,\n",
    "                      ngram_range=(1, 1)\n",
    "\n",
    "#                         lowercase = True,\n",
    "#                         strip_accents='unicode',\n",
    "\n",
    "#                         analyzer='word',\n",
    "#                         token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "#                         ngram_range = (1,1)\n",
    "                      )\n",
    "\n",
    "word_vect = vect.fit_transform(corpus)\n",
    "vect.get_feature_names()\n",
    "# wm2df(cwm, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
