{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from urlextract import URLExtract\n",
    "import matplotlib.pyplot as plt\n",
    "extractor = URLExtract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/Users/collinswestnedge/programming/Metis_Online/project_03/data/jigsaw-toxic-comment-classification-challenge/train.csv')\n",
    "test_df_labels = pd.read_csv('/Users/collinswestnedge/programming/Metis_Online/project_03/data/jigsaw-toxic-comment-classification-challenge/test_labels.csv')\n",
    "test_df = pd.read_csv('/Users/collinswestnedge/programming/Metis_Online/project_03/data/jigsaw-toxic-comment-classification-challenge/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('/Users/collinswestnedge/programming/Metis_Online/project_03/data/tweet-sentiment-extraction/train.csv')\n",
    "# test_df = pd.read_csv('/Users/collinswestnedge/programming/Metis_Online/project_03/data/tweet-sentiment-extraction/test.csv')\n",
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the -1 labels in test labels and \n",
    "# and joining it with the test.csv \n",
    "test_labels_temp = test_df_labels.replace(-1,np.nan)\n",
    "test_labels_clean = test_labels_temp.dropna()\n",
    "test_data = test_df.merge(test_labels_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>Thank you for understanding. I think very high...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002f87b16116a7f</td>\n",
       "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003e1cccfd5a40a</td>\n",
       "      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00059ace3e3e9a53</td>\n",
       "      <td>\" \\n\\n == Before adding a new product to the l...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63973</th>\n",
       "      <td>fff8f64043129fa2</td>\n",
       "      <td>:Jerome, I see you never got around to this…! ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63974</th>\n",
       "      <td>fff9d70fe0722906</td>\n",
       "      <td>==Lucky bastard== \\n http://wikimediafoundatio...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63975</th>\n",
       "      <td>fffa8a11c4378854</td>\n",
       "      <td>==shame on you all!!!== \\n\\n You want to speak...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63976</th>\n",
       "      <td>fffac2a094c8e0e2</td>\n",
       "      <td>MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MO...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63977</th>\n",
       "      <td>fffb5451268fb5ba</td>\n",
       "      <td>\" \\n\\n == Unicorn lair discovery == \\n\\n Suppo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63978 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                       comment_text  \\\n",
       "0      0001ea8717f6de06  Thank you for understanding. I think very high...   \n",
       "1      000247e83dcc1211                   :Dear god this site is horrible.   \n",
       "2      0002f87b16116a7f  \"::: Somebody will invariably try to add Relig...   \n",
       "3      0003e1cccfd5a40a  \" \\n\\n It says it right there that it IS a typ...   \n",
       "4      00059ace3e3e9a53  \" \\n\\n == Before adding a new product to the l...   \n",
       "...                 ...                                                ...   \n",
       "63973  fff8f64043129fa2  :Jerome, I see you never got around to this…! ...   \n",
       "63974  fff9d70fe0722906  ==Lucky bastard== \\n http://wikimediafoundatio...   \n",
       "63975  fffa8a11c4378854  ==shame on you all!!!== \\n\\n You want to speak...   \n",
       "63976  fffac2a094c8e0e2  MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MO...   \n",
       "63977  fffb5451268fb5ba  \" \\n\\n == Unicorn lair discovery == \\n\\n Suppo...   \n",
       "\n",
       "       toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0        0.0           0.0      0.0     0.0     0.0            0.0  \n",
       "1        0.0           0.0      0.0     0.0     0.0            0.0  \n",
       "2        0.0           0.0      0.0     0.0     0.0            0.0  \n",
       "3        0.0           0.0      0.0     0.0     0.0            0.0  \n",
       "4        0.0           0.0      0.0     0.0     0.0            0.0  \n",
       "...      ...           ...      ...     ...     ...            ...  \n",
       "63973    0.0           0.0      0.0     0.0     0.0            0.0  \n",
       "63974    0.0           0.0      0.0     0.0     0.0            0.0  \n",
       "63975    0.0           0.0      0.0     0.0     0.0            0.0  \n",
       "63976    1.0           0.0      1.0     0.0     1.0            0.0  \n",
       "63977    0.0           0.0      0.0     0.0     0.0            0.0  \n",
       "\n",
       "[63978 rows x 8 columns]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.merge(test_df_labels).groupby('toxic').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    \n",
    "    def replace_urls(x):\n",
    "        urls = extractor.find_urls(x)\n",
    "        if urls:\n",
    "            x_new = replace_urls(x.replace(urls[0],''))\n",
    "            return x_new\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    data['comment_text'] = data.comment_text.map(lambda x: replace_urls(x))\n",
    "    #get rid of duplicate letters just trying this out may comment out later\n",
    "    data['comment_text'] = data.comment_text.map(lambda x: re.sub(r'(.)\\1{2,}', '', str(x).lower()))\n",
    "    # removing indents\n",
    "    data['comment_text'] = data.comment_text.map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "    # remove weird user occurence \n",
    "    data['comment_text'] = data.comment_text.map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n",
    "    # remove ip address\n",
    "    data['comment_text'] = data.comment_text.map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
    "    \n",
    "    return data\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import plot_roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(classifier, X_train, y_train, X_test, y_test, plot_curve=False):\n",
    "    \n",
    "    # literally making this for my own practice \n",
    "    # i know theres a lot quicker/simpler ways\n",
    "    \n",
    "    model = classifier\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    actuals = y_test\n",
    "    \n",
    "    print('-'*20 + ' ' + type(model).__name__  + ' ' + '-'*20)\n",
    "        \n",
    "\n",
    "    err_df = pd.DataFrame({'pred':preds, 'actual':actuals})\n",
    "\n",
    "    TP = err_df.actual[(err_df.actual==1) & (err_df.pred==1)].count()\n",
    "    print('TP: ', TP)\n",
    "    # False Positives:\n",
    "    FP = err_df.actual[(err_df.actual==0) & (err_df.pred==1)].count()\n",
    "    print('FP: ', FP)\n",
    "    # #True Negatives:\n",
    "    TN = err_df.actual[(err_df.actual==0) & (err_df.pred==0)].count()\n",
    "    print('TN: ', TN)\n",
    "    #False Negatives:\n",
    "    FN = err_df.actual[(err_df.actual==1) & (err_df.pred==0)].count()\n",
    "    print('FN: ', FN)\n",
    "    print()\n",
    "\n",
    "    precision = TP/(TP+FP)\n",
    "    recall =  TP/(TP+FN)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('Accuracy:', model.score(X_test, y_test))\n",
    "    print('F1 Score:', 2*((precision*recall)/(precision+recall)))\n",
    "    \n",
    "    if plot_curve:\n",
    "        print('Area Under ROC Curve:', roc_auc_score(y_test, model.predict_proba(X_test)[:,1]))\n",
    "        fig, ax = plt.subplots(figsize=(6,4))\n",
    "        test_disp = plot_roc_curve(model, X_test, y_test, ax=ax, color='red', linewidth=3)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = preprocess(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean = preprocess(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- playing with count vectorizer ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# stemmer = PorterStemmer()\n",
    "# lemma = WordNetLemmatizer()\n",
    "# text = train_clean.comment_text.values[40]\n",
    "# token = RegexpTokenizer(r'\\b[a-zA-Z]{3,}\\b')\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# print(text)\n",
    "# print()\n",
    "# print('stemmer',[stemmer.stem(val) for val in word_tokenize(text) if val not in stop_words])\n",
    "# print()\n",
    "# print('lemmatizer', [lemma.lemmatize(val) for val in word_tokenize(text) if val not in stop_words])\n",
    "\n",
    "# print()\n",
    "# print('stemmer',[stemmer.stem(val) for val in token.tokenize(text) if val not in stop_words])\n",
    "# print()\n",
    "# print('lemmatizer', [lemma.lemmatize(val) for val in token.tokenize(text) if val not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# found sorted/ the highest coefficients for both labels and then\n",
    "# looked where the the difference between the \n",
    "new_stop_words = ['anything',\n",
    " 'person',\n",
    " 'day',\n",
    " 'even',\n",
    " 'wrong',\n",
    " 'said',\n",
    " 'personal',\n",
    " 'message',\n",
    " 'site',\n",
    " 'vandalism',\n",
    " 'thing',\n",
    " 'keep',\n",
    " 'right',\n",
    " 'really',\n",
    " 'know',\n",
    " 'make',\n",
    " 'back',\n",
    " 'let',\n",
    " 'put',\n",
    " 'take',\n",
    " 'better',\n",
    " 'something',\n",
    " 'mean',\n",
    " 'say',\n",
    " 'want',\n",
    " 'never',\n",
    " 'think',\n",
    " 'fact',\n",
    " 'time',\n",
    " 'attack',\n",
    " 'warning',\n",
    " 'world',\n",
    " 'blocked',\n",
    " 'still',\n",
    " 'got',\n",
    " 'edits',\n",
    " 'someone',\n",
    " 'way',\n",
    " 'people',\n",
    " 'going',\n",
    " 'well',\n",
    " 'come',\n",
    " 'user',\n",
    " 'one',\n",
    " 'like',\n",
    " 'look',\n",
    " 'change',\n",
    " 'much',\n",
    " 'good',\n",
    " 'tell',\n",
    " 'day',\n",
    " 'even',\n",
    " 'said',\n",
    " 'vandalism',\n",
    " 'thing',\n",
    " 'keep',\n",
    " 'right',\n",
    " 'really',\n",
    " 'know',\n",
    " 'make',\n",
    " 'back',\n",
    " 'let',\n",
    " 'put',\n",
    " 'take',\n",
    " 'better',\n",
    " 'something',\n",
    " 'mean',\n",
    " 'say',\n",
    " 'want',\n",
    " 'think',\n",
    " 'fact',\n",
    " 'time',\n",
    " 'blocked',\n",
    " 'still',\n",
    " 'edits',\n",
    " 'someone',\n",
    " 'way',\n",
    " 'people',\n",
    " 'going',\n",
    " 'well',\n",
    " 'word',\n",
    " 'user',\n",
    " 'one',\n",
    " 'like',\n",
    " 'look',\n",
    " 'change',\n",
    " 'much',\n",
    " 'good',\n",
    " 'comment',\n",
    " 'read',\n",
    " 'many',\n",
    " 'reason',\n",
    " 'sorry',\n",
    " 'page',\n",
    " 'need',\n",
    " 'made',\n",
    " 'edit',\n",
    " 'place',\n",
    " 'name',\n",
    " 'block',\n",
    " 'wikipedia',\n",
    " 'wiki']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_complete = stopwords.words('english') + new_stop_words\n",
    "\n",
    "class Tokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.pt = PorterStemmer()\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.tk = RegexpTokenizer(r'\\b[a-zA-Z]{3,}\\b')\n",
    "        self.stpwrd = set(stop_words_complete)\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in self.tk.tokenize(doc) if not t in self.stpwrd]\n",
    "\n",
    "my_tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.sentiment_neutral.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X, y = train_clean['comment_text'], train_clean['toxic']\n",
    "# # X, y = train_df['text'], train_df['sentiment_positive']\n",
    "\n",
    "# # X, y = train_balanced['comment_text'], train_balanced['toxic']\n",
    "\n",
    "# metrics = []\n",
    "# skf = StratifiedKFold(n_splits=5)\n",
    "# for train_index, test_index in skf.split(X, y):\n",
    "    \n",
    "#     X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "#     y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    \n",
    "#     vect = TfidfVectorizer(\n",
    "\n",
    "# #                           lowercase = True,\n",
    "# #                           token_pattern=None,\n",
    "# #                           tokenizer=my_tokenizer,\n",
    "# #                           ngram_range=(1, 1), \n",
    "# #                           max_features=30000\n",
    "        \n",
    "#                           lowercase = True,\n",
    "#                           strip_accents='unicode',\n",
    "#                           stop_words=stop_words_complete,\n",
    "#                           analyzer='word',\n",
    "#                           token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "#                           ngram_range = (1,1),\n",
    "#                           max_features=8000\n",
    "#                           )\n",
    "    \n",
    "#     X_train_vect = vect.fit_transform(X_train)\n",
    "#     X_val_vect = vect.transform(X_val)\n",
    "    \n",
    "#     nb = MultinomialNB()\n",
    "#     nb.fit(X_train_vect, y_train)\n",
    "\n",
    "#     y_pred_class = nb.predict(X_val_vect)\n",
    "#     metrics.append(accuracy_score(y_val, y_pred_class))\n",
    "\n",
    "# metrics = np.array(metrics)\n",
    "# print('Mean accuracy: ', np.mean(metrics, axis=0))\n",
    "# print('Std for accuracy: ', np.std(metrics, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vectorize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = train_clean['comment_text'], train_clean['toxic']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "\n",
    "vect = TfidfVectorizer(\n",
    "\n",
    "                      lowercase = True,\n",
    "                      token_pattern=None,\n",
    "                      tokenizer=my_tokenizer,\n",
    "                      ngram_range=(1, 1),\n",
    "                      max_features=8000\n",
    "\n",
    "#                       lowercase = True,\n",
    "#                       strip_accents='unicode',\n",
    "#                       stop_words='english',\n",
    "#                       analyzer='word',\n",
    "#                       token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "#                       ngram_range = (1,1),\n",
    "#                       max_features=8000\n",
    "                      )\n",
    "\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "X_val_vect = vect.transform(X_val)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_vect, y_train)\n",
    "nb.score(X_val_vect, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(MultinomialNB(), X_train_vect, y_train, X_val_vect, y_val, plot_curve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_coefs = nb.coef_.reshape(X_vect.shape[1],)\n",
    "# # highest coefficients\n",
    "# idx = model_coefs.argsort()[-100:][::-1]\n",
    "\n",
    "# # lowest coefficients \n",
    "# # idx = model_coefs.argsort()[0:100]\n",
    "\n",
    "# features = vect.get_feature_names()\n",
    "\n",
    "# top_features = [features[idx[i]] for i in range(100)]\n",
    "# top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_features_df = pd.DataFrame({'features': top_features, 'coefficients':model_coefs[idx]})\n",
    "# plt.figure(figsize=[5, 20])\n",
    "# plt.bar(top_features_df.coefficients, top_features_df.features)\n",
    "# # plt.yticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_vect.toarray()[1:10,1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_word_covariance(word_vec, classifier, n=100, original=True):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        get covariance matrix and dataframe for words with\n",
    "        highest coefficients for each class in our trained model\n",
    "        \n",
    "        Paramters:\n",
    "        word_vec: fitted word vector,\n",
    "        classifier: trained classifier (multinomial nb)\n",
    "        n: number of top coefficients\n",
    "        original: default true returns unaltered dataframe as well\n",
    "        \n",
    "        Returns: covariance matrix of top coefficients for each word\n",
    "        dataframe of cofficients for each word.\n",
    "        \"\"\"\n",
    "        \n",
    "        neg_class_prob_sorted = classifier.feature_log_prob_[0, :].argsort()\n",
    "        pos_class_prob_sorted = classifier.feature_log_prob_[1, :].argsort()\n",
    "\n",
    "        negative_class_feats = np.take(word_vec.get_feature_names(), neg_class_prob_sorted[-n:])\n",
    "        positive_class_feats = np.take(word_vec.get_feature_names(), pos_class_prob_sorted[-n:])\n",
    "\n",
    "        neg_coefs = np.take(classifier.feature_log_prob_[0, :],neg_class_prob_sorted[-n:])\n",
    "        pos_coefs = np.take(classifier.feature_log_prob_[1, :],pos_class_prob_sorted[-n:])\n",
    "\n",
    "        pos_dict = dict(zip(positive_class_feats.tolist(), pos_coefs.tolist()))\n",
    "        neg_dict = dict(zip(negative_class_feats.tolist(), neg_coefs.tolist()))\n",
    "        \n",
    "        \n",
    "        if original:\n",
    "            df_temp = pd.DataFrame([neg_dict, pos_dict]).T\n",
    "            df_temp.columns = ['non_toxic_coefs', 'toxic_coefs']\n",
    "            return df_temp\n",
    "        else:\n",
    "        \n",
    "            df = pd.DataFrame([neg_dict, pos_dict]).fillna(0.000001)\n",
    "\n",
    "            df_temp = df.T\n",
    "            df_temp.columns = ['non_toxic_coefs', 'toxic_coefs']\n",
    "\n",
    "            df_scaled = df_temp - df_temp.mean()\n",
    "\n",
    "            return df_scaled, df_scaled.T.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_word_covariance(vect, nb, n=2000)\n",
    "temp = df.reset_index()\n",
    "filtered_temp = temp.sort_values(by=['toxic_coefs'], ascending=False)\n",
    "filtered_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_word_covariance(vect, nb, n=2000)\n",
    "temp = df.reset_index()\n",
    "filtered_temp = temp.sort_values(by=['non_toxic_coefs'], ascending=False)\n",
    "filtered_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get highest coefficients for model\n",
    "# sort dataframe by highest toxic coefficients and THEN\n",
    "# look where coefficients have small difference for both labels\n",
    "# and use this to create stop words\n",
    "\n",
    "df = get_word_covariance(vect, nb, n=2000)\n",
    "temp = df.reset_index()\n",
    "filtered_temp = temp.sort_values(by=['toxic_coefs'], ascending=False)\n",
    "filtered_temp = filtered_temp[(filtered_temp.non_toxic_coefs.notnull()) & (filtered_temp.toxic_coefs.notnull())].head(100).copy()\n",
    "filtered_temp['coefficient_diff'] = (filtered_temp['non_toxic_coefs'] - filtered_temp['toxic_coefs'])**2\n",
    "filtered_temp.sort_values(by=['coefficient_diff'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_word_covariance(vect, nb, n=2000)\n",
    "temp = df.reset_index()\n",
    "filtered_temp = temp.sort_values(by=['non_toxic_coefs'], ascending=False)\n",
    "filtered_temp = filtered_temp[(filtered_temp.non_toxic_coefs.notnull()) & (filtered_temp.toxic_coefs.notnull())].head(100).copy()\n",
    "filtered_temp['coefficient_diff'] = (filtered_temp['non_toxic_coefs'] - filtered_temp['toxic_coefs'])**2\n",
    "filtered_temp.sort_values(by=['coefficient_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df, cov = get_word_covariance(vect, nb, n=1000, original=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import NMF\n",
    "\n",
    "  \n",
    "    \n",
    "# topics = 3\n",
    "# cols = ['topic' + str(i+1) for i in range(topics)]\n",
    "\n",
    "# n_samples = 2000\n",
    "# n_features = 1000\n",
    "# n_components = 5\n",
    "# n_top_words = 20\n",
    "\n",
    "# nmf = NMF(n_components=n_components, random_state=1,\n",
    "#           alpha=.1, l1_ratio=.5).fit(X_vect)\n",
    "\n",
    "\n",
    "# print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "# tfidf_feature_names = vect.get_feature_names()\n",
    "# print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_class_prob_sorted = nb.feature_log_prob_[0, :].argsort()\n",
    "# pos_class_prob_sorted = nb.feature_log_prob_[1, :].argsort()\n",
    "\n",
    "# negative_class_feats = np.take(vect.get_feature_names(), neg_class_prob_sorted[-10:])\n",
    "# positive_class_feats = np.take(vect.get_feature_names(), pos_class_prob_sorted[-10:])\n",
    "\n",
    "# neg_class_prob_sorted[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_features(vector, classifier, n=10, top=True, indices=False):\n",
    "    if top:\n",
    "        neg_class_prob_sorted = nb.feature_log_prob_[0, :].argsort()\n",
    "        pos_class_prob_sorted = nb.feature_log_prob_[1, :].argsort()\n",
    "        \n",
    "        neg_idxs =  neg_class_prob_sorted[-n:]\n",
    "        pos_idxs = pos_class_prob_sorted[-n:]\n",
    "\n",
    "        negative_class_feats = np.take(vect.get_feature_names(), neg_idxs)\n",
    "        positive_class_feats = np.take(vect.get_feature_names(), pos_idxs)\n",
    "        \n",
    "        # return idices for highest coefficients of each class\n",
    "        if indices:\n",
    "            return neg_idxs, pos_idxs\n",
    "        else:\n",
    "            return negative_class_feats, positive_class_feats\n",
    "        \n",
    "    else:\n",
    "        neg_class_prob_sorted = nb.feature_log_prob_[0, :].argsort()\n",
    "        pos_class_prob_sorted = nb.feature_log_prob_[1, :].argsort()\n",
    "\n",
    "        neg_idxs =  neg_class_prob_sorted[:n]\n",
    "        pos_idxs = pos_class_prob_sorted[:n]\n",
    "\n",
    "        negative_class_feats = np.take(vect.get_feature_names(), neg_idxs)\n",
    "        positive_class_feats = np.take(vect.get_feature_names(), pos_idxs)\n",
    "        \n",
    "        if indices:\n",
    "            return neg_idxs, pos_idx\n",
    "        else:\n",
    "            return negative_class_feats, positive_class_feats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "topics = 10\n",
    "cols = ['topic' + str(i+1) for i in range(topics)]\n",
    "nmf = NMF(n_components=topics, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(X_vect)\n",
    "\n",
    "topic_df = pd.DataFrame(nmf.components_, index=cols, columns=vect.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos = get_class_features(vect, nb, n=80, top=True)\n",
    "topic_formatted = topic_df.T[pos].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def graph_topic(words,topic=[1,2]):\n",
    "    topic_formatted = topic_df.T[words].T\n",
    "    if len(topic) == 2:\n",
    "        cols = ['topic'+str(val) for val in topic]\n",
    "        plt.figure(figsize=[5,12])\n",
    "        plt.barh(topic_formatted.index, topic_formatted[cols[0]])\n",
    "        plt.barh(topic_formatted.index, topic_formatted[cols[1]])\n",
    "    else:\n",
    "        print('redo')\n",
    "# plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_topic(pos,[1,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "tfidf_feature_names = vect.get_feature_names()\n",
    "n_top_words = 30\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "\n",
    "topics = 5\n",
    "cols = ['topic' + str(i+1) for i in range(topics)]\n",
    "svd = TruncatedSVD(n_components=topics)\n",
    "lsa = svd.fit_transform(X_vect)\n",
    "lsa_df = pd.DataFrame(lsa, columns=cols)\n",
    "lsa_df['document'] = train_df['comment_text']\n",
    "lsa_df['toxic'] = train_df.toxic\n",
    "lsa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i think i may be indexing into these wrong look/fix later\n",
    "neg, pos = get_class_features(vect, nb, n=20, top=True, indices=True)\n",
    "lsa_df.iloc[neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(svd.components_, index=cols, columns=vect.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(2)\n",
    "# X = pca.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, cov = get_word_covariance(vect, nb, n=100, original=False)\n",
    "cov['gay'].sort_values(ascending=False).to_frame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing = train_df[train_df['comment_text'].str.contains('nazi') & (train_df.toxic==1)].comment_text.values[6]\n",
    "# print(testing)\n",
    "# print('-'*100)\n",
    "# tokenized_test = np.array(my_tokenizer(testing), dtype=np.object)\n",
    "# idx = np.where(tokenized_test=='nazi')[0]\n",
    "\n",
    "# if len(idx) > 0:\n",
    "#     for item in idx:\n",
    "#         if item > 0:\n",
    "#             print(tokenized_test[item-1])\n",
    "#         print(tokenized_test[item])\n",
    "#         if item < len(tokenized_test) -1:\n",
    "#             print(tokenized_test[item+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing = train_df[train_df['comment_text'].str.contains('nazi')&(train_df.toxic==0)].comment_text.values[3]\n",
    "# print(testing[-700::])\n",
    "# print('-'*100)\n",
    "# tokenized_test = np.array(my_tokenizer(testing), dtype=np.object)\n",
    "# idx = np.where(tokenized_test=='nazi')[0]\n",
    "\n",
    "# if len(idx) > 0:\n",
    "#     for item in idx:\n",
    "#         if item > 0:\n",
    "#             print(tokenized_test[item-1])\n",
    "#         print(tokenized_test[item])\n",
    "#         if item < len(tokenized_test) -1:\n",
    "#             print(tokenized_test[item+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- MultinomialNB --------------------\n",
      "TP:  3588\n",
      "FP:  1579\n",
      "TN:  56309\n",
      "FN:  2502\n",
      "\n",
      "Precision: 0.694406812463712\n",
      "Recall: 0.5891625615763547\n",
      "Accuracy: 0.9362124480290099\n",
      "F1 Score: 0.637470018655059\n",
      "Area Under ROC Curve: 0.9470568584508582\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_holdout = test_clean.comment_text\n",
    "y_holdout = test_clean.toxic\n",
    "\n",
    "vect = TfidfVectorizer(\n",
    "\n",
    "                      lowercase = True,\n",
    "                      token_pattern=None,\n",
    "                      tokenizer=my_tokenizer,\n",
    "                      ngram_range=(1, 1),\n",
    "                      max_features=8000\n",
    "\n",
    "#                       lowercase = True,\n",
    "#                       strip_accents='unicode',\n",
    "#                       stop_words='english',\n",
    "#                       analyzer='word',\n",
    "#                       token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "#                       ngram_range = (1,1),\n",
    "#                       max_features=8000\n",
    "                      )\n",
    "\n",
    "X_train_vect = vect.fit_transform(X)\n",
    "X_holdout_vect = vect.transform(X_holdout)\n",
    "\n",
    "get_metrics(MultinomialNB(), X_train_vect, y, X_holdout_vect, y_holdout, plot_curve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so now that we've made a meaninful improvement im going to explore what the model learned... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
